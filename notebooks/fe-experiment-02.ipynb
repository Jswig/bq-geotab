{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Feature Engineering\n",
    "*Anders Poirel*\n",
    "\n",
    "In the previous notebook I realized that one-hot-encoding resulted in a feature matrix that was way to large to be processed in memory, so a new approach was needed. I either need to make my dataloaded batch the dataset OR come up with a more compact encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/raw/train.csv'\n",
    "TEST_PATH = '../data/raw/test.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, 1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['City'] = train['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = X_train['EntryStreetName'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the street names to see if there is some clever feature engineering to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('EntryStreetName')['EntryStreetName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of different entries so we might want to aggregate by street type (we found these types by looking manually throught the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_types = [\n",
    "    'Boulevard', 'Street', 'Avenue', 'Drive', 'Parkway', 'Road', 'Place', 'Way', \n",
    "    'Highway', 'Bridge', 'Tunnel', 'Terrace', 'Square',\n",
    "    'Connector', 'Lane', 'Broadway', 'Wharf', 'Court', 'Circle',\n",
    "]\n",
    "\n",
    "# connector should be tested for before street\n",
    "# street sometimes spelt st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_type(street_name):\n",
    "    street_types = [\n",
    "        'Boulevard', 'Street', 'Avenue', 'Drive', 'Parkway', 'Road', 'Place', 'Way', \n",
    "        'Highway', 'Bridge', 'Tunnel', 'Terrace', 'Square',\n",
    "        'Connector', 'Lane', 'Broadway', 'Wharf', 'Court', 'Circle',\n",
    "        ]\n",
    "    \n",
    "    if pd.isna(street_name):\n",
    "        return 'Not reported'\n",
    "    \n",
    "    # special cases to deal with redundant street names\n",
    "    elif 'St' in street_name:\n",
    "        return 'Street'\n",
    "    elif 'Pkway' in street_name:\n",
    "        return 'Parkway'\n",
    "    \n",
    "    else:\n",
    "        for street_type in street_types:\n",
    "            if street_type in street_name:\n",
    "                return street_type\n",
    "            \n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EntryStreetType'] = X_train['EntryStreetName'].apply(encode_type)\n",
    "X_train['ExitStreetType'] = X_train['ExitStreetName'].apply(encode_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, there are only 1700 different street types, so we might find it interesting to try fitting a model with one-hot-encoding on these, given how some of them appear several thousand times (and so we have a large sample for each category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to D C Achaira's great Kaggle kernel for ideas on building more informative encodings for `EntryHeading` and `ExitHeading`:\n",
    "[Feature Engineering and LightGBM](https://www.kaggle.com/dcaichara/feature-engineering-and-lightgbm)\n",
    "I adapt his method directly here, computing the cardinal entry and exit directions, \n",
    "as well as the difference betweent the two.\n",
    "This encoding is more informative as to similar directions will have more similar encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_direction(direction):\n",
    "    encodings = {\n",
    "        'N': 0,\n",
    "        'NE': 0.25,\n",
    "        'E':  0.5,\n",
    "        'SE': 0.75,\n",
    "        'S': 1,\n",
    "        'SW': 1.25,\n",
    "        'W': 1.5,\n",
    "        'NW': 1.75,\n",
    "    }\n",
    "    return encodings[direction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EntryHeading'] = X_train['EntryHeading'].apply(encode_direction)\n",
    "X_train['ExitHeading'] = X_train['ExitHeading'].apply(encode_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EntryExitDiff'] = X_train['ExitHeading'] - X_train['EntryHeading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['EntryStreetName', 'ExitStreetName', 'Path',\n",
    "              'IntersectionId'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>EntryHeading</th>\n",
       "      <th>ExitHeading</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Month</th>\n",
       "      <th>City</th>\n",
       "      <th>EntryStreetType</th>\n",
       "      <th>ExitStreetType</th>\n",
       "      <th>EntryExitDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>661578</th>\n",
       "      <td>39.96199</td>\n",
       "      <td>-75.15930</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Street</td>\n",
       "      <td>Street</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42200</th>\n",
       "      <td>33.74457</td>\n",
       "      <td>-84.39448</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Street</td>\n",
       "      <td>Street</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617796</th>\n",
       "      <td>40.03954</td>\n",
       "      <td>-75.06014</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597658</th>\n",
       "      <td>39.95726</td>\n",
       "      <td>-75.21531</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Street</td>\n",
       "      <td>Street</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671832</th>\n",
       "      <td>39.93302</td>\n",
       "      <td>-75.14945</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Street</td>\n",
       "      <td>Street</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Latitude  Longitude  EntryHeading  ExitHeading  Hour  Weekend  Month  \\\n",
       "661578  39.96199  -75.15930          1.50         1.50     2        0     10   \n",
       "42200   33.74457  -84.39448          0.50         0.50     6        0      9   \n",
       "617796  40.03954  -75.06014          1.25         1.25    12        1     11   \n",
       "597658  39.95726  -75.21531          0.50         0.50     9        0      8   \n",
       "671832  39.93302  -75.14945          1.50         1.50    14        0     11   \n",
       "\n",
       "                City EntryStreetType ExitStreetType  EntryExitDiff  \n",
       "661578  Philadelphia          Street         Street            0.0  \n",
       "42200        Atlanta          Street         Street            0.0  \n",
       "617796  Philadelphia       Boulevard      Boulevard            0.0  \n",
       "597658  Philadelphia          Street         Street            0.0  \n",
       "671832  Philadelphia          Street         Street            0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will deal will encodings for intersectionID and more advanced feature engineering once I get a baseline model up and working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow boilerplate for preprocessing\n",
    "I'm still learning my way around tensorflow's `feature_columns` and while a lot of this could probably have been done there it's easier for me to do in pandas then convert the results using some boilerplate from `feature_columns` for OHE. I'm sure there's a lot of power I'm not leveraging there though. \n",
    "\n",
    "\n",
    "This boilerplate is similar to that in the previous notebook so I won't comment on it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = train['TotalTimeStopped_p20']\n",
    "y_train_2 = train['TotalTimeStopped_p50']\n",
    "y_train_3 = train['TotalTimeStopped_p80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['Hour', 'Weekend', 'Month', 'City', 'EntryStreetType', 'ExitStreetType']\n",
    "\n",
    "NUMERIC_COLUMNS = ['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'EntryExitDiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(X, y, n_epochs = None, shuffle = False):\n",
    "    def input_f():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y)) \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(len(y))\n",
    "        dataset = dataset.repeat(n_epochs)\n",
    "        dataset = dataset.batch(len(y))\n",
    "        return dataset\n",
    "    return input_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cat_column(feature_name, vocab):\n",
    "    return tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab))\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocabulary = X_train[feature_name].unique()\n",
    "    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "    \n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                                            dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "Let's verify that this all works using the baseline `LinearRegressor` Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.estimator import BoostedTreesRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll import scope as ho_scope\n",
    "from hyperopt.pyll.stochastic import sample as ho_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the objective function for `hyperopt` to optimize, similar to the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv_score(X, y, n_folds, params):\n",
    "    scores = []\n",
    "    model = BoostedTreesRegressor(**params)\n",
    "\n",
    "    for train_index, val_index in KFold(n_splits = 5).split(X):\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        train_input = make_input(X_train, y_train)\n",
    "        val_input = make_input(X_val, y_val)\n",
    "        \n",
    "        model.train(train_input, max_steps = 1)\n",
    "        \n",
    "        # the loss is the mean squared error\n",
    "        result = model.evaluate(val_input)[]\n",
    "        scores.append(tf.math.sqrt(result['average_loss']))\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the search space for hyperopt. I decided on these paramaters by generalizing from my experience in tuning `xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'feature_columns' : feature_columns,\n",
    "    'n_batches_per_layer': 1,\n",
    "    'train_in_memory': True,\n",
    "    'pruning_mode' : 'pre',\n",
    "    'n_trees' : hp.choice('ntrees', [25, 50, 75, 100, 150]),\n",
    "    'max_depth': hp.choice('max_depth', list(range(6,18,2))),\n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.03, 0.12),\n",
    "    'l2_regularization': hp.uniform('l2_regularization', 10, 200)\n",
    "}\n",
    "\n",
    "# try l2 regularization or tree_complexity based on time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_columns': ((('Hour',\n",
       "    (0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4,\n",
       "     5,\n",
       "     6,\n",
       "     7,\n",
       "     8,\n",
       "     9,\n",
       "     10,\n",
       "     11,\n",
       "     12,\n",
       "     13,\n",
       "     14,\n",
       "     15,\n",
       "     16,\n",
       "     17,\n",
       "     18,\n",
       "     19,\n",
       "     20,\n",
       "     21,\n",
       "     22,\n",
       "     23),\n",
       "    tf.int64,\n",
       "    -1,\n",
       "    0),),\n",
       "  (('Weekend', (0, 1), tf.int64, -1, 0),),\n",
       "  (('Month', (6, 7, 8, 9, 10, 11, 12, 1, 5), tf.int64, -1, 0),),\n",
       "  (('City',\n",
       "    ('Atlanta', 'Boston', 'Chicago', 'Philadelphia'),\n",
       "    tf.string,\n",
       "    -1,\n",
       "    0),),\n",
       "  (('EntryStreetType',\n",
       "    ('Boulevard',\n",
       "     'Not reported',\n",
       "     'Street',\n",
       "     'Avenue',\n",
       "     'Road',\n",
       "     'Lane',\n",
       "     'Drive',\n",
       "     'Parkway',\n",
       "     'Place',\n",
       "     'Way',\n",
       "     'Other',\n",
       "     'Highway',\n",
       "     'Terrace',\n",
       "     'Square',\n",
       "     'Circle',\n",
       "     'Connector',\n",
       "     'Broadway',\n",
       "     'Bridge',\n",
       "     'Wharf',\n",
       "     'Court',\n",
       "     'Tunnel'),\n",
       "    tf.string,\n",
       "    -1,\n",
       "    0),),\n",
       "  (('ExitStreetType',\n",
       "    ('Boulevard',\n",
       "     'Street',\n",
       "     'Avenue',\n",
       "     'Road',\n",
       "     'Lane',\n",
       "     'Drive',\n",
       "     'Not reported',\n",
       "     'Parkway',\n",
       "     'Place',\n",
       "     'Way',\n",
       "     'Other',\n",
       "     'Highway',\n",
       "     'Terrace',\n",
       "     'Circle',\n",
       "     'Broadway',\n",
       "     'Square',\n",
       "     'Bridge',\n",
       "     'Wharf',\n",
       "     'Court'),\n",
       "    tf.string,\n",
       "    -1,\n",
       "    0),),\n",
       "  ('Latitude', (1,), None, tf.float32, None),\n",
       "  ('Longitude', (1,), None, tf.float32, None),\n",
       "  ('EntryHeading', (1,), None, tf.float32, None),\n",
       "  ('ExitHeading', (1,), None, tf.float32, None),\n",
       "  ('EntryExitDiff', (1,), None, tf.float32, None)),\n",
       " 'l2_regularization': 148.01897954443288,\n",
       " 'learning_rate': 0.0738746330271447,\n",
       " 'max_depth': 16,\n",
       " 'n_batches_per_layer': 1,\n",
       " 'n_trees': 500,\n",
       " 'pruning_mode': 'pre',\n",
       " 'train_in_memory': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_sample(param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Review how to do hyperopt correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/25 [00:00<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rmse_cv_score() missing 4 required positional arguments: 'y', 'n_folds', 'kwargs', and 'feature_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-ee057ff91f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrials_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_20\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse_cv_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbest_50\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse_cv_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbest_80\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse_cv_Score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    420\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[0;32m    421\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    239\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                         \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    854\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 856\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: rmse_cv_score() missing 4 required positional arguments: 'y', 'n_folds', 'kwargs', and 'feature_columns'"
     ]
    }
   ],
   "source": [
    "trials_reg = Trials()\n",
    "best_20 = fmin(rmse_cv_score, space = param_space, algo = tpe.suggest, max_evals = 25)\n",
    "best_50 = fmin(rmse_cv_score, space = param_space, algo = tpe.suggest, max_evals = 25)\n",
    "best_80 = fmin(rmse_cv_Score, space = param_space, algo = tpe.suggest, max_evals = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Feature Engineering and LightGBM](https://www.kaggle.com/dcaichara/feature-engineering-and-lightgbm)\n",
    "\n",
    "[2] [Tutorial on Hyperopt](https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
