{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Feature Engineering\n",
    "*Anders Poirel*\n",
    "\n",
    "In the previous notebook I realized that one-hot-encoding resulted in a feature matrix that was way to large to be processed in memory, so a new approach was needed. I either need to make my dataloaded batch the dataset OR come up with a more compact encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/raw/train.csv'\n",
    "TEST_PATH = '../data/raw/test.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, 1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['City'] = train['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = X_train['EntryStreetName'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the street names to see if there is some clever feature engineering to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('EntryStreetName')['EntryStreetName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of different entries so we might want to aggregate by street type (we found these types by looking manually throught the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_types = [\n",
    "    'Boulevard', 'Street', 'Avenue', 'Drive', 'Parkway', 'Road', 'Place', 'Way', \n",
    "    'Highway', 'Bridge', 'Tunnel', 'Terrace', 'Square',\n",
    "    'Connector', 'Lane', 'Broadway', 'Wharf', 'Court', 'Circle',\n",
    "]\n",
    "\n",
    "# connector should be tested for before street\n",
    "# street sometimes spelt st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_type(street_name):\n",
    "    street_types = [\n",
    "        'Boulevard', 'Street', 'Avenue', 'Drive', 'Parkway', 'Road', 'Place', 'Way', \n",
    "        'Highway', 'Bridge', 'Tunnel', 'Terrace', 'Square',\n",
    "        'Connector', 'Lane', 'Broadway', 'Wharf', 'Court', 'Circle',\n",
    "        ]\n",
    "    \n",
    "    if pd.isna(street_name):\n",
    "        return 'Not reported'\n",
    "    \n",
    "    # special cases to deal with redundant street names\n",
    "    elif 'St' in street_name:\n",
    "        return 'Street'\n",
    "    elif 'Pkway' in street_name:\n",
    "        return 'Parkway'\n",
    "    \n",
    "    else:\n",
    "        for street_type in street_types:\n",
    "            if street_type in street_name:\n",
    "                return street_type\n",
    "            \n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EntryStreetType'] = X_train['EntryStreetName'].apply(encode_type)\n",
    "X_train['ExitStreetType'] = X_train['ExitStreetName'].apply(encode_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, there are only 1700 different street types, so we might find it interesting to try fitting a model with one-hot-encoding on these, given how some of them appear several thousand times (and so we have a large sample for each category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to D C Achaira's great Kaggle kernel for ideas on building more informative encodings for `EntryHeading` and `ExitHeading`:\n",
    "[Feature Engineering and LightGBM](https://www.kaggle.com/dcaichara/feature-engineering-and-lightgbm)\n",
    "I adapt his method directly here, computing the cardinal entry and exit directions, \n",
    "as well as the difference betweent the two.\n",
    "This encoding is more informative as to similar directions will have more similar encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_direction(direction):\n",
    "    encodings = {\n",
    "        'N': 0,\n",
    "        'NE': 0.25,\n",
    "        'E':  0.5,\n",
    "        'SE': 0.75,\n",
    "        'S': 1,\n",
    "        'SW': 1.25,\n",
    "        'W': 1.5,\n",
    "        'NW': 1.75,\n",
    "    }\n",
    "    return encodings[direction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EntryHeading'] = X_train['EntryHeading'].apply(encode_direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ExitHeading'] = X_train['ExitHeading'].apply(encode_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EntryExitDiff'] = X_train['ExitHeading'] - X_train['EntryHeading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['EntryStreetName', 'ExitStreetName', 'Path',\n",
    "              'IntersectionId'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>EntryHeading</th>\n",
       "      <th>ExitHeading</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Month</th>\n",
       "      <th>City</th>\n",
       "      <th>EntryStreetType</th>\n",
       "      <th>ExitStreetType</th>\n",
       "      <th>EntryExitDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.79166</td>\n",
       "      <td>-84.43003</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.79166</td>\n",
       "      <td>-84.43003</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.79166</td>\n",
       "      <td>-84.43003</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.79166</td>\n",
       "      <td>-84.43003</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.79166</td>\n",
       "      <td>-84.43003</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>Boulevard</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude  Longitude  EntryHeading  ExitHeading  Hour  Weekend  Month  \\\n",
       "0  33.79166  -84.43003          1.75         1.75     0        0      6   \n",
       "1  33.79166  -84.43003          0.75         0.75     0        0      6   \n",
       "2  33.79166  -84.43003          1.75         1.75     1        0      6   \n",
       "3  33.79166  -84.43003          0.75         0.75     1        0      6   \n",
       "4  33.79166  -84.43003          1.75         1.75     2        0      6   \n",
       "\n",
       "      City EntryStreetType ExitStreetType  EntryExitDiff  \n",
       "0  Atlanta       Boulevard      Boulevard            0.0  \n",
       "1  Atlanta       Boulevard      Boulevard            0.0  \n",
       "2  Atlanta       Boulevard      Boulevard            0.0  \n",
       "3  Atlanta       Boulevard      Boulevard            0.0  \n",
       "4  Atlanta       Boulevard      Boulevard            0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will deal will encodings for intersectionID and more advanced feature engineering once I get a baseline model up and working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow boilerplate for preprocessing\n",
    "I'm still learning my way around tensorflow's `feature_columns` and while a lot of this could probably have been done there it's easier for me to do in pandas then convert the results using some boilerplate from `feature_columns` for OHE. I'm sure there's a lot of power I'm not leveraging there though. \n",
    "\n",
    "\n",
    "This boilerplate is similar to that in the previous notebook so I won't comment on it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = train['TotalTimeStopped_p20']\n",
    "y_train_2 = train['TotalTimeStopped_p50']\n",
    "y_train_3 = train['TotalTimeStopped_p80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['Hour', 'Weekend', 'Month', 'City', 'EntryStreetType', 'ExitStreetType']\n",
    "\n",
    "NUMERIC_COLUMNS = ['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'EntryExitDiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(X, y, n_epochs = None, shuffle = False):\n",
    "    def input_f():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y)) \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(len(y))\n",
    "        dataset = dataset.repeat(n_epochs)\n",
    "        dataset = dataset.batch(len(y))\n",
    "        return dataset\n",
    "    return input_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cat_column(feature_name, vocab):\n",
    "    return tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab))\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocabulary = X_train[feature_name].unique()\n",
    "    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "    \n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                                            dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "Let's verify that this all works using the baseline `LinearRegressor` Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Anders\\AppData\\Local\\Temp\\tmpiufzb3sa\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Anders\\\\AppData\\\\Local\\\\Temp\\\\tmpiufzb3sa', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BB067A7108>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:518: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Anders\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\ftrl.py:143: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Anders\\AppData\\Local\\Temp\\tmpiufzb3sa\\model.ckpt.\n",
      "INFO:tensorflow:loss = 53.12389, step = 0\n"
     ]
    }
   ],
   "source": [
    "model_linear = tf.estimator.LinearRegressor(feature_columns)\n",
    "input_fn = make_input(X_train, y_train_1)\n",
    "model_linear.train(input_fn, max_steps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model_linear.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt as hp\n",
    "from tensorflow.estimator import BoostedTreesRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the objective function for `hyperopt` to optimize, similar to the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv_score(X, y, n_folds, kwargs, feature_columns):\n",
    "    scores = []\n",
    "    model = BoostedTreesRegressor(feature_columns, n_batches_per_layer = 1, **kwargs)\n",
    "\n",
    "    for train_index, val_index in KFold(n_splits = 5).split(X):\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        train_input = make_input(X_train, y_train)\n",
    "        val_input = make_input(X_val, y_val)\n",
    "        \n",
    "        model.train(train_input, max_steps = 1)\n",
    "        \n",
    "        scores.append(tf.math.sqrt(model.evaluate(val_input)))\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the search space for hyperopt. I decided on these paramaters by generalizing from my experience in tuning `xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
